#!/usr/bin/env python
# coding: utf-8

# Script that can be used to check the code quality metrics for both front-end and back-end code.
# Author: Xander Smeets (1325523)
#
# This script needs 2 inputs, specified as such: python understand_verify.py <metrics.csv> <true/false>
# where metrics.csv is a code quality metrics file generated by Understand and the second parameter
# is true for object-oriented languages and false for non-object-oriented languages

import numpy as np
import pandas as pd
import sys
import logging
import math

# Assert that the second input is either true or false:
if not (sys.argv[2] == 'true' or sys.argv[2] == 'false'):
    exit(2)

# Based on https://stackoverflow.com/a/27674608/2378368
# This should clean up some of the unnecessary information provided when Exceptions occur.


def exception_handler(exception_type, exception, traceback):
    # All your trace are belong to us!
    # your format
    print("%s: %s" % (exception_type.__name__, exception))


sys.excepthook = exception_handler

# Variable to store whether any test has failed
exit_code = 0

# Set up logger
logger = logging.getLogger('Java')

# Constants which define thresholds
if sys.argv[2] == 'true':
    METRIC_NAME_NUM_METHODS = 'CountDeclMethod'
else:
    METRIC_NAME_NUM_METHODS = 'CountDeclFunction'
THRESHOLD_SLOC = 400
THRESHOLD_AVG_COMPLEXITY = 10
THRESHOLD_MAX_COMPLEXITY = 20
THRESHOLD_NUM_METHODS_IN_CLASS = 20
THRESHOLD_PERCENT_LINES_COMMENT = 0.15
THRESHOLD_CYCLIC_DEPENDENCIES = 0
THRESHOLD_COUPLING = 16


# Function to retrieve the name of a class
def class_name(metrics_classes, i):
    return metrics_classes.loc[i, 'Name']


# Verifier for module size (i.e. software lines of code)
def verify_module_size(metrics_classes, failures_allowed):
    numberErrors = 0
    for i in range(metrics_classes.shape[0]):
        try:
            if metrics_classes.loc[i, 'CountLineCode'] > THRESHOLD_SLOC:
                raise Exception("Class " + class_name(metrics_classes, i) + " has " + str(
                    metrics_classes.loc[i, 'CountLineCode']) + " lines of code, which is more than the allowed " + str(THRESHOLD_SLOC) + ".")
        except Exception as e:
            logger.error(str(e))
            numberErrors += 1
    if (numberErrors > failures_allowed):
        global exit_code
        exit_code = 1



# Verifier for average cyclomatic complexity
def verify_avg_complexity(metrics_classes, failures_allowed):
    numberErrors = 0
    for i in range(metrics_classes.shape[0]):
        try:
            if metrics_classes.loc[i, 'AvgCyclomatic'] >= THRESHOLD_AVG_COMPLEXITY:
                raise Exception("Class " + class_name(metrics_classes, i) + " has " + str(
                    metrics_classes.loc[i, 'AvgCyclomatic']) + " average cyclomatic complexity, which is more than the allowed " + str(THRESHOLD_AVG_COMPLEXITY) + ".")
        except Exception as e:
            logger.error(str(e))
            numberErrors += 1
    if (numberErrors > failures_allowed):
        global exit_code
        exit_code = 1


# Verifier for maximum cyclomatic complexity
def verify_max_complexity(metrics_classes, failures_allowed):
    numberErrors = 0
    for i in range(metrics_classes.shape[0]):
        try:
            if metrics_classes.loc[i, 'MaxCyclomatic'] >= THRESHOLD_MAX_COMPLEXITY:
                raise Exception("Class " + class_name(metrics_classes, i) + " has " + str(
                    metrics_classes.loc[i, 'MaxCyclomatic']) + " maximum cyclomatic complexity, which is more than the allowed " + str(THRESHOLD_MAX_COMPLEXITY) + ".")
        except Exception as e:
            logger.error(str(e))
            numberErrors += 1
    if (numberErrors > failures_allowed):
        global exit_code
        exit_code = 1



# Verifier for number of methods in a single class
def verify_max_num_methods_in_class(metrics_classes, failures_allowed):
    numberErrors = 0
    for i in range(metrics_classes.shape[0]):
        try:
            if metrics_classes.loc[i, METRIC_NAME_NUM_METHODS] > THRESHOLD_NUM_METHODS_IN_CLASS:
                raise Exception("Class " + class_name(metrics_classes, i) + " has declared " + str(
                    metrics_classes.loc[i, METRIC_NAME_NUM_METHODS]) + " methods, which is more than the allowed " + str(THRESHOLD_NUM_METHODS_IN_CLASS) + ".")
        except Exception as e:
            logger.error(str(e))
            numberErrors += 1
    if (numberErrors > failures_allowed):
        global exit_code
        exit_code = 1



# Verifier for the percentage of lines that are comments
def verify_percent_lines_comments(metrics_classes, failures_allowed):
    numberErrors = 0
    for i in range(metrics_classes.shape[0]):
        try:
            if metrics_classes.loc[i, 'RatioCommentToCode'] <= THRESHOLD_PERCENT_LINES_COMMENT:
                raise Exception("Class " + class_name(metrics_classes, i) + " has " + str(
                    metrics_classes.loc[i, 'RatioCommentToCode'] * 100) + "% lines of comments, which is less than the acceptable " + str(THRESHOLD_PERCENT_LINES_COMMENT * 100) + "%.")
        except Exception as e:
            logger.error(str(e))
            numberErrors += 1
    if (numberErrors > failures_allowed):
        global exit_code
        exit_code = 1



# Verifier for amount of coupling
def verify_coupling(metrics_classes, failures_allowed):
    numberErrors = 0
    for i in range(metrics_classes.shape[0]):
        try:
            if metrics_classes.loc[i, 'CountClassCoupled'] >= THRESHOLD_COUPLING:
                raise Exception("Class " + class_name(metrics_classes, i) + " is coupled to " + str(
                    metrics_classes.loc[i, 'CountClassCoupled']) + " other classes, which is more than the allowed " + str(THRESHOLD_COUPLING-1) + ".")
        except Exception as e:
            logger.error(str(e))
            numberErrors += 1
    if (numberErrors > failures_allowed):
        global exit_code
        exit_code = 1



# Import data from earlier CI job
metrics = pd.read_csv(sys.argv[1])
# Only consider classes
if sys.argv[2] == 'true':
    metrics_classes = metrics[metrics['Kind'].str.contains('Class', case=False)].reset_index()
else:
    metrics_classes = metrics[metrics['Kind'].str.contains('File', case=False)].reset_index()


allowed_to_fail = math.floor(metrics_classes.shape[0] * 0.03)

# Actual check execution
verify_module_size(metrics_classes, allowed_to_fail)
verify_avg_complexity(metrics_classes, allowed_to_fail)
verify_max_complexity(metrics_classes, allowed_to_fail)
verify_max_num_methods_in_class(metrics_classes, allowed_to_fail)
verify_percent_lines_comments(metrics_classes, allowed_to_fail)
if sys.argv[2] == 'true':
    verify_coupling(metrics_classes, allowed_to_fail)

# Return to GitLab CI whether tests have passed
exit(exit_code)
